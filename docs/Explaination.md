# outside
## Arena.py
竞技场文件。
**__init__**初始化要提供两个对比的模型，游戏，然后有一个显示函数的空槽。
**playGames**然后可以调用playGames进行竞技，要输入跑的盘数和是否显示。输入的盘数应该是偶数，双方会先下一半，然后交换棋子再跑另一半。最后输出总的双方胜的和负的。
**playGame**则是跑一局的代码。

## Args.py
**iteration**这里提供了对于训练流程的绝大多数参数的设置，除了网络参数的部分设置在NNet.py中。包括总的跑多少轮，每一轮获取多少局自博奕数据，历史经验池的大小，每一次收集数据后训练多少步(这里是因为AlphaZero的原始设计是并行的收集数据和训练，每1000步会比较一次，这里我们资源受限，所以串行进行，先收集一轮数据，然后进行1000步训练，然后测试)，Batchsize大小，收集一轮数据最大的容量，
**MCTS**每一步要做多少次MTCS搜索，设置在MCTS树根部注入噪声的参数。
**Evaluation**用于进化比较的局数和用于Elo分数计算的局数设置。进化胜率要求。
**archive**存档的位置和是否恢复的标志以及恢复要读取的权重文件名。
**draw**和棋的参数包括多少步的限制，多少步没有进展的限制，和棋给的值(由于原项目设置0为继续没有中止的标志，所以如果我给了0则可能产生bug于一些规则有冲突，为了稳定考虑，我设置平局为一个极小值)。
**alphabeta**生成几轮alphabeta生成的老师数据。

## Coach.py
**__init__**教练需要输入网络架构，参数，游戏。为了能够加速训练我还提供了一个参数表示利用alphabeta树模型生成几轮数据供网络训练，相当于一个老师，如果不需要，而要沿着第一性原理，做完全的自学习则直接不设，默认为0，相关的代码就完全不会调用了。同时这个参数也作为之后删除经验池中旧历史的位置指示，即当当前最优模型的Elo分数小于等于1500，即最优模型不如两层的alphabeta树时，不会删去它生成的数据，而是先删去自博奕的旧历史。
**executeEpisode**中是自博奕生成一局历史的函数，内部会获取概率分布，然后记录操作和下棋的选手，然后执行，在最后出结果后一个个处理得到训练数据。
**executeEpisodeab**则是在上者基础上修改，实现的是利用alphabeta树生成数据。
**learn**是会先生成alphabeta的数据，存储到变量中，然后进行正常的自博奕训练流程。其中每一轮会先积累特定局数的数据，然后如果池子容量满了就会删除旧数据，同时要考虑到如果设置了获取alphabeta的老师数据，同时没有超过老师会先删自己的。然后保存数据到磁盘。再加载模型，开始训练新模型，然后开始竞技得到的胜率如果超过了旧模型则会替换当前的最优模型，同时记录Tensorboard相应的值，最后记录这一轮的情况以供断点续训的恢复工作。
**getCheckpointFile**获取保存的文件名
**saveTrainExamples**保存训练数据到磁盘中
**loadTrainExamples**加载训练数据到变量中
**save_iteration_checkpoints**保存用于断点续训的恢复的元数据，即当前的轮数，最佳的Elo分数，Tensorboard所写的路径，这样能够完整恢复当前状态，包括第几轮，训练数据，当前最优Elo分数，同时可以继续接着记录Tensorboard。
**load_iteration_checkpoints**读取存的元数据到resume.pt，然后存到变量中，同时存储一份供其他函数调用。

## mainTafl.py
新建一个游戏，然后新建一个网络。再根据是否加载网络的参数决定是否加载网络的权重，再新建教练，然后加载训练历史数据池，加载元数据。最后开始学习。

## pitTafl.py
用于测试的文件，比较几个模型的性能，这里原始项目提供了随机，贪心和人交互三种模型，我也重新提供了随机，贪心和alphabeta树三种模型，都在baselines中，可以创建后在这里统一进行测试比较



# baselines
## alphabeta_player.py
alphabeta树代理，
**__init__**输入游戏和树的深度参数。
**_legal_actions**根据当前棋盘获取合法操作。
**_term_origin**根据当前棋盘和这一步原始棋手(即要真正下在棋盘中的，而不是模拟中的棋手)，然后获取这个棋手的终局值。
**_eval_origin**根据当前棋盘和原始棋手，然后得到原始棋手视角的棋盘，然后调用函数获取当前局面的分数，这部分是手工编码的。
**_ab**获取当前终局值，如果有结果或者已经抵达设置的深度，则返回终局值或者对当前局面的值。然后获取合法动作，然后根据当前下棋的是否和原始棋手是一方做不同处理，是一方则递归处理所有合法动作，然后取最大的结果，记录，同时另一方则是取最小的结果，这是基础的最大最小树。同时还有alphabeta剪枝处理，alpha是和原始同阵营维护，其值只能从上往下传而不能返回，所以alpha记录的是从最上面到自己这一条线自己作为需要最大化阵营能够得到的最大的值，同时beta则是自己上一步的对手能够选择的让我方最小的值，如果alpha>=beta则对手肯定不会选自己，因为有更少的可以选；同样的beta是对手在维护，如果对方的beta都已经比我当前能够获得的alpha即最好值小我也肯定不会选那一步，所以能够剪枝。最后返回从自己视角的最优解和最优值。
**__call__**提供了调用的接口，输入棋盘，调用处理，然后返回操作。
**getActionProb**这里是用alphabeta树做训练数据，所以先会执行所有合法动作，得到结果，如果有直接胜的则返回独热，如果只有平局则均分概率返回。然后对所有操作做ab树递归处理得到返回，然后得到概率分布返回。

## Elo_Cal.py
提供和2层alphabeta树比较计算Elo分数的工具函数。
**elo_vs_ab**根据胜，输，平和对手的基数Elo分数计算我们的Elo分数以及误差分数上下限，胜率。
**Evaluate_Model_with_Alpha_Beta**提供新模型，游戏，进行评估的局数，是否写入Tensorboard。然后进行测试，得到结果获取分数，然后记录。

## greedy_player.py
**__call**它会对于每一步执行操作后用启发式结果评估执行后棋盘的值，然后在最优值中随机选。
**_eval_board**这里的启发式也供alphabeta使用，其会根据王是否死亡，王和边沿的距离，自己和对方的棋子数量差，最后按照一定权重组合得到对于当前棋盘的评估。

## random_player.py
**__call**对于所有合法动作随机选择。

## TaflPlayer.py
原始项目提供的随机，贪心(直接用子数差做启发式)，人可以对弈的agent。



# father_class
## Game.py
## NeuralNet.py
各种游戏和网络的父类。



# models
## MAZM.py
网络的设计参考AlphaZero的设计，对于这个游戏修改了动作空间，减小了residual块的数量。这里网络的设计和AlphaZero更接近，即一个格子的上下左右8格的空间，但是由于原项目使用的是从任意一个位置到任意一个位置的设置，非常不合理，但是所有游戏规则都基于此，所以我对这个网络做了两个包装，一是输出时恢复到外部的动作空间，二是训练时训练数据到这种小的合理空间的转化。

## MCTS.py
**__init**记录当前游戏，网络，参数，维护MCTS值，如当前根的总值，边访问了多少次，整个访问了多少次，初始从网络获取的概率分布，记录当前状态是否终止，是否合法。
**getActionProb**输入棋盘后进行搜索的总流程控制，首先对棋盘进行表达做MCTS的键值，然后进行n步搜索，首轮搜索会注入噪声，根据合法动作空间大小动态分配噪声值，使得网络自博奕时做更多的探索。在最后，搜索完成后得到每个合法动作进行了多少次访问，然后根据温度，如果是0则直接选择最优的为1，反之根据温度使分布更平滑，然后返回整个概率分布。
**search**根据当前棋盘进行递归的搜索，直到终结。首先获取这个局面的键值表示，然后如果以前没有到过这个节点就做这个状态的初值记录：包括终局值，网络给的各个动作概率分布的先验，合法动作空间，更新后的此状态的概率分布(根据合法掩码)，记录合法动作掩码，当前访问次数0，返回网络输出的价值。然后如果已经访问过了，计算合法动作的u值，得到有最大u值的节点，然后执行得到下一步棋盘状态和棋手然后做下一步搜索获取值，更新这个动作访问次数加1，更新当前节点获取的平均价值。更新这个节点访问次数加1。返回价值。所有价值的返回都要取反，因为是给对手提供值。

## NNet.py
提供对于网络的包装，实现训练，预测，保存等功能。
**_round_lr_lambda**提供对于学习率的控制，满足我对于30轮的调节需求。
**__init**获取网络，棋盘尺寸，外部动作空间，两个满足网络由于动作空间要转化需求的表，优化器和调度器。优化器和调度器维护用于在多个循环都对网络进行优化。
**train**输入训练数据，Batchsize，训练多少步，当前循环序号。首先调度学习率，得到棋盘大小，数据增强对应表，制作数据集，得到损失函数。然后开启训练，对于数据先解包，然后将数据转化到小空间，输入网络得到结果计算损失优化。
**predict**输入棋盘，转化到6维的网络输出，然后输入网络拿到输出的动作的概率分布和状态的价值。然后将概率分布转化回大空间返回。
剩下的是损失函数和网络权重的保存和加载。
## Players.py
对于MCTS的包装使得可以在Arena方便的调用。
## RandomData.py
**__init__**在初始化中输入转化动作概率分布的表，训练数据，棋盘大小。对于每个训练数据整理一下存起来。
**__getitem__**取出对象，随机进行8中数据增强的一种(旋转4种和镜像2种相乘)，然后转化为6维的数据输入网络，然后组合输出。



# Rules
## GameVariants.py
原项目提供的各种变体，但是我仅仅对于tablut进行了处理，特殊空间仅有王座。但仅仅给8分之一的棋子位置，然后**expandeighth**进行了3次翻转得到棋子和特殊地形的两个集合。
## TaflGame.py
**__init__**输入名字来指定特定规则的初始化棋盘。
**getInitBoard**根据名字获取特定的初始化棋盘，然后返回。
**getBoardSize**获取棋盘的尺寸。
**getActionSize**获取动作空间，这里是外部的，即从任意一个位置到任意一个位置。
**getActionSize_net**给网络的动作空间，即一个位置上下左右各8格的小动作空间。
**getNextState**对于棋盘执行动作后返回新棋盘，棋手取反。
**getValidMoves**获取合法动作掩码，然后做单一值到四坐标值的转化。
**getGameEnded**检测是否可走，如果不可则直接判负，如果可以看当前维护的终局值转化为自己视角。
**getCanonicalForm**获取自己视角的棋盘，实际仅仅是给了两个标志是否翻转棋子值，是否翻转王值。这个不影响实际的值，而是在后面转化为矩阵，以及后续转化为字符串后的值会根据自己的视角转换，因为规则等根据棋子值判定如果修改会可能出bug，所以只是修改最终显示出的值，然后影响MCTS和网络的键值，使得模型能够判定是谁面对这个棋局。
**getSymmetries**翻转和镜像的数据增强，在后面数据集中实现，这里实现会增加时间开销，同时增加了数据的相关性，不好。
**stringRepresentation**获取棋盘的字符串表示，直接调用。
**BoardRepresentation**获取棋盘的纯粹表示，仅有网络形状和是谁在下是开始实现3次循环判定时在外部判定所以需要传输这种表示，但是现在已经废弃因为已经将这个实现放到棋盘内部判定，更优雅。
**getScore**获取分数，如果结束会直接得到终局值1000倍再转化视角，如果没有结束，直接用棋子数差的简单启发式。

**cell_str**对不同值对应不同的图标。
**display**对于棋盘打印出来做一个可视化，可以在对弈时快速看到结果。

## TaflLogic.py
这里设置了棋盘和完整的规则设置。
**__init__**维护棋盘尺寸，棋盘的王座等地形和棋子，当前进行步数，终局值(没有终局则值为0)，是否翻转棋子和王，目前剩余的棋子数，用于处理三次循环规则的对象，首先记录原始棋盘。
**__str__**对于当前局面的表示，记录棋盘局面，谁在下，多少步没有进展，进行了多少步，终局值。终局值是必须要的，因为同一局面可能是因为其他原因平局，也可能正常进行。而对于加进展值和步数唯一的顾虑是使得树稀疏，但是不用担心，因为同一局面的值是通过网络来提供的，网络通过学习可以降低这里稀疏无法利用同局面的风险，而增加了这些值可以让网络因此根据不同情况比如更加紧迫要超限而使用更激进的方式，提供了这种信息。
**__getitem__**获取显示对象
**astype**转化为np表示。
**getPBR**获取纯表示用于记录处理三次循环平局规则。
**getCopy**复制棋盘，先获得游戏，然后获得所有的值，其中三次循环中的对象应该是深拷贝，不然会全局用一个。
**countDiff**计算双方子数差，简单的启发式，用于原项目的其他agent。
**get_legal_moves**获取合法动作。
**has_legal_moves**检测是否有合法动作，同时获取合法动作检查长度。
**execute_move**根据动作先获取要移动的棋子序号，然后检查移动是否合法，如果合法则移动。
**getImage**获取矩阵表示，可用于转化为字符串，作为键值使用。
**getPlayerToMove**检查是谁在下，直接通过下的步数检查。
**_isLegalMove**检查移动是否合理，一步步看，看范围，棋子是否死亡(死亡是通过设置x坐标为-99)，是否走直线，是否移动，是否是移动自己的棋子，是否移动到禁落点王座，是否经过禁落点王座，是否路径过了棋子。
**_getCaptures**检查这一步吃掉的子，检查所有的棋子，如果是敌方棋子看是否挨着，如果挨着看对面是否有自己的棋子夹着，或者如果王不在王座看对面是否是王座，如果满足规则可以吃则记录这个棋子。
**_threefold**检查三次循环平局，首先让翻转到1的视角，然后输入纯表示，然后转化成原视角，然后是否平局。
**_moveByPieceNo**移动，仍然再次检查是否合法，防止有人调用内部输入不合法动作导致问题。然后增加步数，获取棋子，然后得到能吃的棋子，然后将这些棋子杀死(x设为-99)，检查是否有进展，获取终局值，如果没有结束则再检查是否有三次循环情况，如果有则设为平局值。
**_has_progress**更新进展值，如果棋子少了则更新棋子数，然后增加没有进展的值。
**_getWinLose**先检查是否过限制步数或者没有进展过限制步数，有则直接判平局。然后检测王是否到达边缘，到达则判胜，反之则继续，如果没有找到王则白方判负。
**_getPieceNo**根据坐标遍历判断棋子序号。
**_getValidMoves**对于每个棋子，如果是相同阵营则在四个方向所有可能遍历检查是否合法返回。

## test_rules.py
早期对于游戏规则的简单测试，尤其一些规则增加后可能出问题。



# utils
## Digits.py
**int2base**从整数值即0-N^4的动作空间转化到四元坐标动作空间。
**base2int**从四元坐标动作空间转化到整数。

## log.py
通过了日志记录和Tensorboard的初始化，然后提供给其他文件使用。

## smaller_actionspace.py
**_actually_build_maps**获取大小空间的转化，按照一定顺序从左下角到右下角得到大小两种表示然后记录到对应表中。
**build_maps**获取一次则固定下来。得到大小空间转化的表，不可更改。
**expand_small_to_big_probs**根据表将小空间动作概率分布转化到大空间。根据表映射。
**compress_big_to_small_pi**根据表将大空间动作概率分布转化到小空间，同样根据表映射。归一化一次。

## ThreefoldRepetition.py
用于判断是否有三次循环，利用一个Counter()，将string键值转化为16字节然后如果有三次的则返回True。

## utils.py
**AverageMeter**方便的更新一系列数的平均值。
**dotdict**方便的访问参数。
**_rot_xy_vec**做旋转的坐标转化。
**action_perms**对于动作空间中的0-n^4，一一做转化然后制作成表，缓存供调用。
**getNNImage**将原本的getImage的矩阵表示手动分解而不用网络先学习如何分解，分解成6层，分别是双方的棋子，自己的王和对方的王(两种情况，有或没有)，王座，剩余步数比例，整合成一个np数组。
